{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chief-journalist"
      },
      "source": [
        "# Linear Regression with Regularization"
      ],
      "id": "chief-journalist"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arabic-shaft"
      },
      "source": [
        "## Problem Statement"
      ],
      "id": "arabic-shaft"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prospective-thread"
      },
      "source": [
        "Predict the bike-sharing counts per hour based on the features including weather, day, time, humidity, wind speed, season e.t.c."
      ],
      "id": "prospective-thread"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "organic-christmas"
      },
      "source": [
        "## Objectives"
      ],
      "id": "organic-christmas"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phantom-begin"
      },
      "source": [
        "* perform data exploration and visualization\n",
        "* implement linear regression using sklearn and optimization\n",
        "* apply regularization on regression using Lasso, Ridge and Elasticnet techniques\n",
        "* calculate and compare the MSE value of each regression technique\n",
        "* analyze the features that are best contributing to the target"
      ],
      "id": "phantom-begin"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "received-quilt"
      },
      "source": [
        "### Dataset"
      ],
      "id": "received-quilt"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sporting-replication"
      },
      "source": [
        "The dataset chosen for this project is [Bike Sharing Dataset](https://archive.ics.uci.edu/ml/datasets/bike+sharing+dataset).  This dataset contains the hourly and daily count of rental bikes between the years 2011 and 2012 in the capital bike share system with the corresponding weather and seasonal information. This dataset consists of 17389 instances of 16 features. "
      ],
      "id": "sporting-replication"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "binary-evening"
      },
      "source": [
        "### Data Fields\n",
        "\n",
        "* dteday - hourly date\n",
        "* season - 1 = spring, 2 = summer, 3 = fall, 4 = winter\n",
        "* hr - hour\n",
        "* holiday - whether the day is considered a holiday\n",
        "* workingday - whether the day is neither a weekend nor holiday\n",
        "* weathersit -<br>\n",
        "    1 - Clear, Few clouds, Partly cloudy, Partly cloudy <br>\n",
        "    2 - Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist<br>\n",
        "    3 - Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds<br>\n",
        "    4 - Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog<br>   \n",
        "* temp - temperature in Celsius\n",
        "* atemp - \"feels like\" temperature in Celsius\n",
        "* humidity - relative humidity\n",
        "* windspeed - wind speed\n",
        "* casual - number of non-registered user rentals initiated\n",
        "* registered - number of registered user rentals initiated\n",
        "* cnt - number of total rentals"
      ],
      "id": "binary-evening"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download the dataset"
      ],
      "metadata": {
        "id": "yFZ5z0mkBIQI"
      },
      "id": "yFZ5z0mkBIQI"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cross-survivor"
      },
      "source": [
        "#### Importing Necessary Packages"
      ],
      "id": "cross-survivor"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ethical-essence"
      },
      "source": [
        "# Loading the Required Packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn import linear_model\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.metrics import r2_score"
      ],
      "id": "ethical-essence",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "through-scotland"
      },
      "source": [
        "### Data Loading"
      ],
      "id": "through-scotland"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "comic-consolidation"
      },
      "source": [
        "# Read the hour.csv file\n",
        "df = pd.read_csv('hour.csv')"
      ],
      "id": "comic-consolidation",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ordered-overall"
      },
      "source": [
        "print the first five rows of dataset"
      ],
      "id": "ordered-overall"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exterior-committee"
      },
      "source": [
        "df.head(5)"
      ],
      "id": "exterior-committee",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e67gbVvLIsoS"
      },
      "source": [
        "print the datatypes of the columns"
      ],
      "id": "e67gbVvLIsoS"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sharp-shelter"
      },
      "source": [
        "df.dtypes"
      ],
      "id": "sharp-shelter",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opponent-block"
      },
      "source": [
        "### Task flow with respect to feature processing and model training\n",
        "\n",
        "* Explore and analyze the data\n",
        "\n",
        "* Identify continuous features and categorical features\n",
        "\n",
        "* Apply scaling on continuous features and one-hot encoding on categorical features\n",
        "\n",
        "* Separate the features, targets and split the data into train and test\n",
        "\n",
        "* Find the coefficients of the features using normal equation and find the cost (error)\n",
        "\n",
        "* Apply batch gradient descent technique and find the best coefficients\n",
        "\n",
        "* Apply SGD Regressor using sklearn\n",
        "\n",
        "* Apply linear regression using sklearn\n",
        "\n",
        "* Apply Lasso, Ridge, Elasticnet Regression"
      ],
      "id": "opponent-block"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "magnetic-penny"
      },
      "source": [
        "### EDA &  Visualization "
      ],
      "id": "magnetic-penny"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "constitutional-techno"
      },
      "source": [
        "#### Visualize the hour (hr) column and find the busy hours of bike sharing"
      ],
      "id": "constitutional-techno"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uniform-comfort"
      },
      "source": [
        "plt.bar(df['hr'], df['cnt'])\n",
        "plt.show()\n"
      ],
      "id": "uniform-comfort",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flexible-export"
      },
      "source": [
        "#### Visualize the distribution of count, casual and registered variables"
      ],
      "id": "flexible-export"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "manufactured-introduction"
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(8,8))\n",
        "sns.distplot(df['cnt'])"
      ],
      "id": "manufactured-introduction",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.histplot(df['cnt'].sort_values(ascending=False))"
      ],
      "metadata": {
        "id": "kL-hN-Ah5Kpd"
      },
      "id": "kL-hN-Ah5Kpd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "powerful-involvement"
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(8,8))\n",
        "sns.distplot(df['casual'])"
      ],
      "id": "powerful-involvement",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inside-consideration"
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(8,8))\n",
        "sns.distplot(df['registered'])"
      ],
      "id": "inside-consideration",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Describe the relation of weekday, holiday and working day"
      ],
      "metadata": {
        "id": "aX6eW3JhCiDD"
      },
      "id": "aX6eW3JhCiDD"
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = df[['weekday', 'holiday', 'workingday']]\n",
        "df1.corr()"
      ],
      "metadata": {
        "id": "puMD2O4lCmZJ"
      },
      "id": "puMD2O4lCmZJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thrown-allowance"
      },
      "source": [
        "#### Visualize the month wise count of both casual and registered for the year 2011 and 2012 separately."
      ],
      "id": "thrown-allowance"
    },
    {
      "cell_type": "code",
      "source": [
        "df['dteday'] = pd.to_datetime(df['dteday'])"
      ],
      "metadata": {
        "id": "B62_HT-i8B7d"
      },
      "id": "B62_HT-i8B7d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "4On63ZPcZHla"
      },
      "id": "4On63ZPcZHla",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp1 = df[df['yr'] == 0]\n",
        "temp2 = df[df['yr'] == 1]"
      ],
      "metadata": {
        "id": "yiHTkeL6ALz8"
      },
      "id": "yiHTkeL6ALz8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(figsize=(12,8))\n",
        "temp1[['mnth', 'casual', 'registered']].groupby(by='mnth').count().plot(kind='bar', stacked=True, ax=ax)"
      ],
      "metadata": {
        "id": "ETQ8QqqxG2CY"
      },
      "id": "ETQ8QqqxG2CY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joined-court"
      },
      "source": [
        "# stacked bar chart for year 2012\n",
        "fig, ax = plt.subplots(figsize=(12,8))\n",
        "temp2[['mnth', 'casual', 'registered']].groupby(by='mnth').count().plot(kind='bar', stacked=True, ax=ax)"
      ],
      "id": "joined-court",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fifty-driver"
      },
      "source": [
        "#### Analyze the correlation between features with heatmap"
      ],
      "id": "fifty-driver"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "instant-coalition"
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(20,10))\n",
        "sns.heatmap(df.corr(), annot=True)"
      ],
      "id": "instant-coalition",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pursuant-diary"
      },
      "source": [
        "#### Visualize the box plot of casual and registered variables to check the outliers"
      ],
      "id": "pursuant-diary"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stainless-robert"
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(10,10))\n",
        "sns.boxplot(data=df, y='casual', ax=ax)\n"
      ],
      "id": "stainless-robert",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(figsize=(10,15))\n",
        "sns.boxplot(data=df, y='registered', ax=ax)\n"
      ],
      "metadata": {
        "id": "vKgeKeDDJ56K"
      },
      "id": "vKgeKeDDJ56K",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "comparative-heritage"
      },
      "source": [
        "### Pre-processing and Data Engineering "
      ],
      "id": "comparative-heritage"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "massive-scotland"
      },
      "source": [
        "#### Drop unwanted columns"
      ],
      "id": "massive-scotland"
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(5)"
      ],
      "metadata": {
        "id": "U-w2Tv1nKA8w"
      },
      "id": "U-w2Tv1nKA8w",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "appreciated-lawyer"
      },
      "source": [
        "df = df.drop(columns=['instant', 'dteday'])"
      ],
      "id": "appreciated-lawyer",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nASeXE_7JC0L"
      },
      "source": [
        "#### Identify categorical and continuous variables\n"
      ],
      "id": "nASeXE_7JC0L"
    },
    {
      "cell_type": "code",
      "source": [
        "categorical = []\n",
        "numerical = []\n",
        "for cols in df.columns:\n",
        "  if df[cols].nunique() <= 15:\n",
        "    categorical.append(cols)\n",
        "  else:\n",
        "    numerical.append(cols)"
      ],
      "metadata": {
        "id": "7GOrtVEzLbYv"
      },
      "id": "7GOrtVEzLbYv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in categorical:\n",
        "  if col =='season':\n",
        "    categorical.remove(col)\n",
        "    numerical.append(col)\n",
        "  if col =='weathersit':\n",
        "    categorical.remove(col)\n",
        "    numerical.append(col)"
      ],
      "metadata": {
        "id": "7oIdJGIUPGVM"
      },
      "id": "7oIdJGIUPGVM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "corrected-concrete"
      },
      "source": [
        "#### Feature scaling\n",
        "\n",
        "Apply scaling on the continuous variables on the given data.\n",
        "\n"
      ],
      "id": "corrected-concrete"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deadly-leisure"
      },
      "source": [
        "sc = StandardScaler()\n",
        "df[numerical] = sc.fit_transform(df[numerical])"
      ],
      "id": "deadly-leisure",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "min_max = MinMaxScaler()\n",
        "df[numerical] = min_max.fit_transform(df[numerical])"
      ],
      "metadata": {
        "id": "lj9Oe1X2PyDW"
      },
      "id": "lj9Oe1X2PyDW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "located-emperor"
      },
      "source": [
        "#### Apply one-hot encode on the categorical data"
      ],
      "id": "located-emperor"
    },
    {
      "cell_type": "code",
      "source": [
        "ohe = OneHotEncoder()\n",
        "hour_df_copy = df.copy()\n",
        "#hour_df_ohe = pd.get_dummies(hour_df_copy, columns=['season', 'mnth', 'hr', 'weathersit'], prefix = ['season', 'mnth', 'hr', 'weathersit'])\n",
        "hour_df_ohe = pd.get_dummies(df, columns = categorical)\n",
        "hour_df_ohe.columns"
      ],
      "metadata": {
        "id": "Et44o0n_C8zx"
      },
      "id": "Et44o0n_C8zx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# encoder = OneHotEncoder(handle_unknown='ignore')\n",
        "# encoder.fit(df[categorical])\n",
        "# category_columns_encoded = encoder.fit_transform(df[categorical]).toarray()\n",
        "# category_cols_df = pd.DataFrame(category_columns_encoded, columns=encoder.get_feature_names(categorical))\n",
        "# df_encoded = pd.concat([df.drop(columns=categorical), category_cols_df], axis=1)"
      ],
      "metadata": {
        "id": "qnvV84oRVeD_"
      },
      "id": "qnvV84oRVeD_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hour_df_ohe.head()"
      ],
      "metadata": {
        "id": "AL0pdmi1QYWv"
      },
      "id": "AL0pdmi1QYWv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "straight-teens"
      },
      "source": [
        "#### Specify features and targets after applying scaling and one-hot encoding"
      ],
      "id": "straight-teens"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "civic-private"
      },
      "source": [
        "features = hour_df_ohe.drop(columns=['cnt'])\n",
        "target = hour_df_ohe[['cnt']]"
      ],
      "id": "civic-private",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apparent-restoration"
      },
      "source": [
        "### Implement the linear regression by finding the coefficients using below approaches\n",
        "\n",
        "* Find the coefficients using normal equation\n",
        "\n",
        "* (Optional) Implement batch gradient descent \n",
        "\n",
        "* (Optional) SGD Regressor from sklearn"
      ],
      "id": "apparent-restoration"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "involved-shame"
      },
      "source": [
        "#### Select the features and target and split the dataset\n",
        "\n",
        "As there are 3 target variables, choose the count (`cnt`) variable."
      ],
      "id": "involved-shame"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "excess-interview"
      },
      "source": [
        "X = hour_df_ohe[features]\n",
        "y = hour_df_ohe[target]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X.values, y.values, test_size=0.2, random_state=42)\n",
        "print(X_train.shape, y_train.shape, '\\n', X_test.shape, y_test.shape)"
      ],
      "id": "excess-interview",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bA2i6mvbf5yZ"
      },
      "source": [
        "#### Implementation using Normal Equation\n",
        "\n",
        "$\\theta = (X^T X)^{-1} . (X^T Y)$\n",
        "\n",
        "$Î¸$ is the hypothesis parameter that defines the coefficients\n",
        "\n",
        "$X$ is the input feature value of each instance\n",
        "\n",
        "$Y$ is Output value of each instance"
      ],
      "id": "bA2i6mvbf5yZ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66vjMjeHUTGO"
      },
      "source": [
        "theta = np.linalg.inv(X_train.T.dot(X_train)).dot(X_train.T).dot(y_train)\n",
        "theta"
      ],
      "id": "66vjMjeHUTGO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfHHTPUl6joI"
      },
      "source": [
        "#### Implementing Linear regression using batch gradient descent\n",
        "\n",
        "Initialize the random coefficients and optimize the coefficients in the iterative process by calculating cost and finding the gradient."
      ],
      "id": "JfHHTPUl6joI"
    },
    {
      "cell_type": "code",
      "source": [
        "def cost_function(X, Y, B):\n",
        " m = len(Y)\n",
        " J = np.sum((X.dot(B) - Y) ** 2)/2*m\n",
        " return J"
      ],
      "metadata": {
        "id": "btHvSdC_XOcZ"
      },
      "id": "btHvSdC_XOcZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_gradient_descent(X, Y, B, alpha, iterations):\n",
        " cost_history = [0] * iterations\n",
        " m = len(Y)\n",
        " \n",
        " for iteration in range(iterations):\n",
        "    #print(iteration)\n",
        "    # Hypothesis Values\n",
        "    h = X.dot(B)\n",
        "    # Difference b/w Hypothesis and Actual Y\n",
        "    loss = h - Y\n",
        "    # Gradient Calculation\n",
        "    gradient = X.T.dot(loss) / m\n",
        "    # Changing Values of B using Gradient\n",
        "    B = B - (alpha * gradient)\n",
        "    # New Cost Value\n",
        "    cost = cost_function(X, Y, B)\n",
        "    cost_history[iteration] = cost\n",
        "  \n",
        " return B, cost_history"
      ],
      "metadata": {
        "id": "4ibYSZZEXOe6"
      },
      "id": "4ibYSZZEXOe6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQAmuH2FmXWu"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(features.values, target.values, test_size= 0.2, random_state= 0)"
      ],
      "id": "vQAmuH2FmXWu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train"
      ],
      "metadata": {
        "id": "PXcdpHPNbRDG"
      },
      "id": "PXcdpHPNbRDG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = np.c_[np.ones(len(X_train),dtype='int64'),X_train]\n",
        "X_test = X_test = np.c_[np.ones(len(X_test),dtype='int64'),X_test]"
      ],
      "metadata": {
        "id": "Xdne1Cq-bTaL"
      },
      "id": "Xdne1Cq-bTaL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape, y_train.shape"
      ],
      "metadata": {
        "id": "myUUIi16N_wk"
      },
      "id": "myUUIi16N_wk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "B = np.zeros(X_train.shape[1])\n",
        "alpha = 0.1\n",
        "iter_ = 50\n",
        "newB, cost_history = batch_gradient_descent(X_train, y_train.ravel(), B, alpha, iter_)"
      ],
      "metadata": {
        "id": "cmItrj-waO63"
      },
      "id": "cmItrj-waO63",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "newB"
      ],
      "metadata": {
        "id": "stFVzwBdaAcf"
      },
      "id": "stFVzwBdaAcf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmZJdg33NlvA"
      },
      "source": [
        "#### SGD Regressor\n",
        "\n",
        "* Import SGDRegressor from sklearn and fit the data\n",
        "\n",
        "* Predict the test data and find the error"
      ],
      "id": "PmZJdg33NlvA"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fU1-w4XRNlLA"
      },
      "source": [
        "from sklearn.linear_model import SGDRegressor\n",
        "sgd_reg = SGDRegressor(max_iter=1000, penalty=None, eta0=0.1)\n",
        "# Fit the model to the training data\n",
        "sgd_reg.fit(X_train, y_train.ravel())"
      ],
      "id": "fU1-w4XRNlLA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on the test data\n",
        "y_pred = sgd_reg.predict(X_test)\n",
        "y_pred"
      ],
      "metadata": {
        "id": "Iqk150yLEz9M"
      },
      "id": "Iqk150yLEz9M",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the mean squared error of the predictions\n",
        "from sklearn.metrics import mean_squared_error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Mean squared error:\", mse)"
      ],
      "metadata": {
        "id": "WFvLDlpFE3H4"
      },
      "id": "WFvLDlpFE3H4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "legal-engagement"
      },
      "source": [
        "### Linear regression using sklearn\n",
        "\n",
        "Implement the linear regression model using sklearn\n",
        "\n",
        "* Import Linear Regression and fit the train data\n",
        "\n",
        "* Predict the test data and find the error"
      ],
      "id": "legal-engagement"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "complicated-reserve"
      },
      "source": [
        "lr = linear_model.LinearRegression()\n",
        "lr.fit(X_train, y_train)"
      ],
      "id": "complicated-reserve",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = lr.predict(X_test)"
      ],
      "metadata": {
        "id": "wMUizEtIPxOf"
      },
      "id": "wMUizEtIPxOf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred"
      ],
      "metadata": {
        "id": "nt3SDSWcRlR4"
      },
      "id": "nt3SDSWcRlR4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quality-morgan"
      },
      "source": [
        "#### Calculate the $R^2$ (coefficient of determination) of the actual and predicted data"
      ],
      "id": "quality-morgan"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "important-jacket"
      },
      "source": [
        "r2_score(y_pred, y_test)"
      ],
      "id": "important-jacket",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "infinite-victim"
      },
      "source": [
        "#### Summarize the importance of features\n",
        "\n",
        "Use the coefficients obtained through the sklearn Linear Regression implementation and create a bar chart of the coefficients."
      ],
      "id": "infinite-victim"
    },
    {
      "cell_type": "code",
      "source": [
        "lr.coef_.shape"
      ],
      "metadata": {
        "id": "YCdue0hqSHFv"
      },
      "id": "YCdue0hqSHFv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "coef_list = []\n",
        "for coef in lr.coef_[0]:\n",
        "  coef_list.append(coef)"
      ],
      "metadata": {
        "id": "cI_0EYDzS1M3"
      },
      "id": "cI_0EYDzS1M3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.shape(coef_list)"
      ],
      "metadata": {
        "id": "EJ8FWpHgT1De"
      },
      "id": "EJ8FWpHgT1De",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "affected-walker"
      },
      "source": [
        "lr_coef = lr.coef_\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12,9))\n",
        "plt.bar([x for x in range(len(coef_list))], coef_list)"
      ],
      "id": "affected-walker",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "convinced-snowboard"
      },
      "source": [
        "### Regularization methods"
      ],
      "id": "convinced-snowboard"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twenty-italic"
      },
      "source": [
        "#### Apply Lasso regression\n",
        "\n",
        "* Apply Lasso regression with different alpha values given below and find the best alpha that gives the least error.\n",
        "* Calculate the metrics for the actual and predicted"
      ],
      "id": "twenty-italic"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psychological-blake"
      },
      "source": [
        "# setting up alpha\n",
        "alphas = [0.0001, 0.001,0.01, 0.1, 1, 10, 100]"
      ],
      "id": "psychological-blake",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "existing-sigma"
      },
      "source": [
        "for alpha in alphas:\n",
        "    model = linear_model.Lasso(alpha=alpha)\n",
        "    model.fit(X_train, y_train)\n",
        "    score = model.score(X_test, y_test)\n",
        "    print(\"Alpha = {}, R2 = {}\".format(alpha, score))"
      ],
      "id": "existing-sigma",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "figured-effectiveness"
      },
      "source": [
        "#### Apply Ridge regression\n",
        "\n",
        "* Apply Ridge regression with different alpha values given and find the best alpha that gives the least error.\n",
        "* Calculate the metrics for the actual and predicted"
      ],
      "id": "figured-effectiveness"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "curious-initial"
      },
      "source": [
        "for alpha in alphas:\n",
        "    model = linear_model.Ridge(alpha=alpha)\n",
        "    model.fit(X_train, y_train)\n",
        "    score = model.score(X_test, y_test)\n",
        "    print(\"Alpha = {}, R2 = {}\".format(alpha, score))"
      ],
      "id": "curious-initial",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exposed-bottom"
      },
      "source": [
        "#### Apply Elasticnet regression\n",
        "\n",
        "* Apply Elasticnet regression with different alpha values given and find the best alpha that gives the least error.\n",
        "* Calculate the metrics for the actual and predicted"
      ],
      "id": "exposed-bottom"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shared-belief"
      },
      "source": [
        "for alpha in alphas:\n",
        "    model = linear_model.ElasticNet(alpha=alpha)\n",
        "    model.fit(X_train, y_train)\n",
        "    score = model.score(X_test, y_test)\n",
        "    print(\"Alpha = {}, R2 = {}\".format(alpha, score))"
      ],
      "id": "shared-belief",
      "execution_count": null,
      "outputs": []
    }
  ]
}